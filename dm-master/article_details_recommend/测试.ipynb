{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache c:\\users\\wangwe~1\\appdata\\local\\temp\\jieba.cache\n",
      "Loading model cost 0.524 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import jieba\n",
    "jieba.initialize()\n",
    "\n",
    "# data = pd.read_csv(\"./off_line_file/article_data_v5_2017-03-01-20-56.txt\", sep=\"\\t\", encoding=\"utf8\")\n",
    "input_file = \"./off_line_file/article_data_v4_2017-02-17-16-59_part.csv\"\n",
    "data = pd.read_csv(input_file, sep=\"\\t\", encoding=\"gbk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%timeit\n",
    "# %timeit \n",
    "data[u\"pro_id1\"] = data[u\"pro_id\"].fillna(value=-1).map(lambda id: str(id) if id != -1 else \"\")\n",
    "vect_word.fit_transform(data[\"pro_id1\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%timeit\n",
    "data[u\"pro_id2\"] = data[u\"pro_id\"].where(data[u\"pro_id\"].notnull(), None)\n",
    "transform(data[\"pro_id2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"./off_line_file/train_data/article_data_v5_2017-03-01-20-56.txt\", sep=\"\\t\", encoding=\"utf8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 loops, best of 3: 33.8 ms per loop\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "\n",
    "data[u\"pro_id\"] = data[u\"pro_id\"].fillna(value=\"\").astype(unicode)\n",
    "data[u\"level_1_id\"] = data[u\"level_1_id\"].fillna(value=\"\").astype(unicode)\n",
    "data[u\"level_2_id\"] = data[u\"level_2_id\"].fillna(value=\"\").astype(unicode)\n",
    "data[u\"level_3_id\"] = data[u\"level_3_id\"].fillna(value=\"\").astype(unicode)\n",
    "data[u\"level_4_id\"] = data[u\"level_4_id\"].fillna(value=\"\").astype(unicode)\n",
    "data[u\"brand_id\"] = data[u\"brand_id\"].fillna(value=\"\").astype(unicode)\n",
    "\n",
    "cols = [u\"pro_id\", u\"level_1_id\", u\"level_2_id\", u\"level_3_id\", u\"level_4_id\", u\"brand_id\"]\n",
    "for col in cols:\n",
    "    vect = MyCountVectorizer(stop_words=(\"\", \"0\", \"-1\"))\n",
    "    vect.fit_transform(data[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 loops, best of 3: 83.3 ms per loop\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "\n",
    "data[u\"pro_id\"] = data[u\"pro_id\"].fillna(value=-1).map(lambda id: str(id) if id != -1 else \"\")\n",
    "data[u\"level_1_id\"] = data[u\"level_1_id\"].map(lambda id: str(id) if id != -1 else \"\")\n",
    "data[u\"level_2_id\"] = data[u\"level_2_id\"].map(lambda id: str(id) if id != -1 else \"\")\n",
    "data[u\"level_3_id\"] = data[u\"level_3_id\"].map(lambda id: str(id) if id != -1 else \"\")\n",
    "data[u\"level_4_id\"] = data[u\"level_4_id\"].map(lambda id: str(id) if id != -1 else \"\")\n",
    "data[u\"brand_id\"] = data[u\"brand_id\"].fillna(value=-1).map(lambda id: str(id) if id != -1 else \"\")\n",
    "\n",
    "cols = [u\"pro_id\", u\"level_1_id\", u\"level_2_id\", u\"level_3_id\", u\"level_4_id\", u\"brand_id\"]\n",
    "for col in cols:\n",
    "    vect = CountVectorizer(analyzer=\"word\")\n",
    "    vect.fit_transform(data[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 loop, best of 3: 395 ms per loop\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vect_word = CountVectorizer(analyzer=\"word\")\n",
    "\n",
    "# data[u\"pro_id\"] = data[u\"pro_id\"].fillna(value=-1).map(lambda id: str(id) if id != -1 else \"\")\n",
    "# data[u\"level_1_id\"] = data[u\"level_1_id\"].map(lambda id: str(id) if id != -1 else \"\")\n",
    "# data[u\"level_2_id\"] = data[u\"level_2_id\"].map(lambda id: str(id) if id != -1 else \"\")\n",
    "# data[u\"level_3_id\"] = data[u\"level_3_id\"].map(lambda id: str(id) if id != -1 else \"\")\n",
    "# data[u\"level_4_id\"] = data[u\"level_4_id\"].map(lambda id: str(id) if id != -1 else \"\")\n",
    "# data[u\"brand_id\"] = data[u\"brand_id\"].fillna(value=-1).map(lambda id: str(id) if id != -1 else \"\")\n",
    "# data[u\"level_3_id_is_null\"] = np.where(data[u\"level_3_id\"] == \"\", 1, 0)\n",
    "\n",
    "\n",
    "# vect_word.fit_transform(data[u\"pro_id\"])\n",
    "# vect_word.fit_transform(data[u\"level_1_id\"])\n",
    "# vect_word.fit_transform(data[u\"level_2_id\"])\n",
    "# vect_word.fit_transform(data[u\"level_3_id\"])\n",
    "# vect_word.fit_transform(data[u\"level_4_id\"])\n",
    "# vect_word.fit_transform(data[u\"brand_id\"])\n",
    "data[u\"title\"] = data[u\"title\"].map(lambda s: re.sub(\"\\n\", \" \", s.upper()) if s else \"\")\n",
    "sen = \"\\n\".join(data[u\"title\"].values)\n",
    "data[u\"title_fc\"] = \",\".join(jieba.cut(sen)).split(\"\\n\")  # 对标题进行分词\n",
    "# vect_word.fit_transform(data[u\"title_fc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import array\n",
    "from collections import defaultdict, Mapping\n",
    "\n",
    "import re\n",
    "# import jieba\n",
    "# jieba.initialize()\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "\n",
    "from numpy import frombuffer as frombuffer_empty\n",
    "from tools.stop_words import SIGNAL_STOP_WORDS, ENGLISH_STOP_WORDS\n",
    "\n",
    "def _make_int_array():\n",
    "    \"\"\"Construct an array.array of a type suitable for scipy.sparse indices.\"\"\"\n",
    "    return array.array(str(\"i\"))\n",
    "\n",
    "class MyCountVectorizer(object):\n",
    "    \"\"\"\n",
    "    将文档集转为一个 token 计数矩阵，矩阵采用 scipy.sparse.coo_matrix 稀疏矩阵。.\n",
    "    文档集中每个文档都只包含一个词或者一个数字。\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "\n",
    "    stop_words : string {'english'}, list, or None (default)\n",
    "        If 'english', a built-in stop word list for English is used.\n",
    "\n",
    "        If a list, that list is assumed to contain stop words, all of which\n",
    "        will be removed from the resulting tokens.\n",
    "        Only applies if ``analyzer == 'word'``.\n",
    "\n",
    "        If None, no stop words will be used. max_df can be set to a value\n",
    "        in the range [0.7, 1.0) to automatically detect and filter stop\n",
    "        words based on intra corpus document frequency of terms.\n",
    "\n",
    "    vocabulary : Mapping or iterable, optional\n",
    "        Either a Mapping (e.g., a dict) where keys are terms and values are\n",
    "        indices in the feature matrix, or an iterable over terms. If not\n",
    "        given, a vocabulary is determined from the input documents. Indices\n",
    "        in the mapping should not be repeated and should not have any gap\n",
    "        between 0 and the largest index.\n",
    "\n",
    "    binary : boolean, default=False\n",
    "        If True, all non zero counts are set to 1. This is useful for discrete\n",
    "        probabilistic models that model binary events rather than integer\n",
    "        counts.\n",
    "\n",
    "    dtype : type, optional\n",
    "        Type of the matrix returned by fit_transform() or transform().\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    vocabulary_ : dict\n",
    "        A mapping of terms to feature indices.\n",
    "\n",
    "    stop_words_ : set\n",
    "        Terms that were ignored because they either:\n",
    "\n",
    "          - occurred in too many documents (`max_df`)\n",
    "          - occurred in too few documents (`min_df`)\n",
    "          - were cut off by feature selection (`max_features`).\n",
    "\n",
    "        This is only available if no vocabulary was given.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    The ``stop_words_`` attribute can get large and increase the model size\n",
    "    when pickling. This attribute is provided only for introspection and can\n",
    "    be safely removed using delattr or set to None before pickling.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, stop_words=None,\n",
    "                 vocabulary=None, binary=False,\n",
    "                 dtype=np.int8):\n",
    "        self.stop_words = stop_words\n",
    "        self.vocabulary = vocabulary\n",
    "        self.binary = binary\n",
    "        self.dtype = dtype\n",
    "\n",
    "    def _validate_vocabulary(self):\n",
    "        vocabulary = self.vocabulary\n",
    "        # vocabulary 不为空\n",
    "        if vocabulary is not None:\n",
    "            if isinstance(vocabulary, set):\n",
    "                vocabulary = sorted(vocabulary)\n",
    "            if not isinstance(vocabulary, Mapping):\n",
    "                vocab = {}\n",
    "                for i, t in enumerate(vocabulary):\n",
    "                    if vocab.setdefault(t, i) != i:\n",
    "                        msg = \"Duplicate term in vocabulary: %r\" % t\n",
    "                        raise ValueError(msg)\n",
    "                vocabulary = vocab\n",
    "            else:\n",
    "                indices = set(vocabulary.itervalues())\n",
    "                if len(indices) != len(vocabulary):\n",
    "                    raise ValueError(\"Vocabulary contains repeated indices.\")\n",
    "                for i in xrange(len(vocabulary)):\n",
    "                    if i not in indices:\n",
    "                        msg = (\"Vocabulary of size %d doesn't contain index \"\n",
    "                               \"%d.\" % (len(vocabulary), i))\n",
    "                        raise ValueError(msg)\n",
    "            if not vocabulary:\n",
    "                raise ValueError(\"empty vocabulary passed to fit\")\n",
    "            self.fixed_vocabulary_ = True\n",
    "            self.vocabulary_ = dict(vocabulary)\n",
    "        else:\n",
    "            self.fixed_vocabulary_ = False\n",
    "\n",
    "    def _sort_features(self, X, vocabulary):\n",
    "        \"\"\"Sort features by name\n",
    "\n",
    "        Returns a reordered matrix and modifies the vocabulary in place\n",
    "        \"\"\"\n",
    "        sorted_features = sorted(vocabulary.iteritems())\n",
    "        map_index = np.empty(len(sorted_features), dtype=np.int32)\n",
    "        for new_val, (term, old_val) in enumerate(sorted_features):\n",
    "            map_index[new_val] = old_val\n",
    "            vocabulary[term] = new_val\n",
    "        return X[:, map_index]\n",
    "\n",
    "    def _count_vocab(self, raw_documents, fixed_vocab):\n",
    "        \"\"\"Create sparse feature matrix, and vocabulary where fixed_vocab=False\n",
    "        \"\"\"\n",
    "        if fixed_vocab:\n",
    "            vocabulary = self.vocabulary_\n",
    "        else:\n",
    "            # Add a new value when a new vocabulary item is seen\n",
    "            vocabulary = defaultdict()\n",
    "            vocabulary.default_factory = vocabulary.__len__\n",
    "\n",
    "        j_indices = []\n",
    "        indptr = _make_int_array()\n",
    "        values = _make_int_array()\n",
    "        indptr.append(0)\n",
    "\n",
    "        for doc in raw_documents:\n",
    "            if self.stop_words:\n",
    "                if doc in self.stop_words:\n",
    "                    indptr.append(len(j_indices))\n",
    "                    continue\n",
    "            feature_counter = {}\n",
    "            try:\n",
    "                feature_idx = vocabulary[doc]\n",
    "                if feature_idx not in feature_counter:\n",
    "                    feature_counter[feature_idx] = 1\n",
    "                else:\n",
    "                    feature_counter[feature_idx] += 1\n",
    "            except KeyError:\n",
    "                # 使用自定义词库表时，如果文档中包含的词汇[特征]不在自定义词库表中，\n",
    "                # 则不再将该词汇[特征]加入到词库表\n",
    "                # Ignore out-of-vocabulary items for fixed_vocab=True\n",
    "                continue\n",
    "\n",
    "            j_indices.extend(feature_counter.keys())\n",
    "            values.extend(feature_counter.values())\n",
    "            indptr.append(len(j_indices))\n",
    "\n",
    "        # 自动生成[学习]词库表的情况\n",
    "        if not fixed_vocab:\n",
    "            # disable defaultdict behaviour\n",
    "            vocabulary = dict(vocabulary)\n",
    "            if not vocabulary:\n",
    "                raise ValueError(\"empty vocabulary; perhaps the documents only\"\n",
    "                                 \" contain stop words\")\n",
    "\n",
    "        j_indices = np.asarray(j_indices, dtype=np.intc)\n",
    "        indptr = np.frombuffer(indptr, dtype=np.intc)\n",
    "        values = frombuffer_empty(values, dtype=np.intc)\n",
    "\n",
    "        X = sp.csr_matrix((values, j_indices, indptr),\n",
    "                          shape=(len(indptr) - 1, len(vocabulary)),\n",
    "                          dtype=self.dtype)\n",
    "        #\n",
    "        X.sort_indices()\n",
    "        return vocabulary, X\n",
    "\n",
    "    def fit_transform(self, raw_documents):\n",
    "        \"\"\"Learn the vocabulary dictionary and return term-document matrix.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        raw_documents : iterable\n",
    "            An iterable which yields either str, unicode or file objects.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        X : array, [n_samples, n_features]\n",
    "            Document-term matrix.\n",
    "        \"\"\"\n",
    "        if isinstance(raw_documents, str):\n",
    "            raise ValueError(\n",
    "                \"Iterable over raw text documents expected, \"\n",
    "                \"string object received.\")\n",
    "\n",
    "        # 校验vocabulary，如果之前提供了自定义vocabulary，则将self.fixed_vocabulary_设为True，否则False\n",
    "        self._validate_vocabulary()\n",
    "\n",
    "        vocabulary, X = self._count_vocab(raw_documents,\n",
    "                                          self.fixed_vocabulary_)\n",
    "\n",
    "        if self.binary:\n",
    "            X.data.fill(1)\n",
    "\n",
    "        # 自动生成[学习]词库表的情况\n",
    "        if not self.fixed_vocabulary_:\n",
    "            X = self._sort_features(X, vocabulary)\n",
    "            self.vocabulary_ = vocabulary\n",
    "        return X\n",
    "\n",
    "    def inverse_transform(self, X):\n",
    "        \"\"\"Return terms per document with nonzero entries in X.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array, sparse matrix}, shape = [n_samples, n_features]\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        X_inv : list of arrays, len = n_samples\n",
    "            List of arrays of terms.\n",
    "        \"\"\"\n",
    "\n",
    "        if sp.issparse(X):\n",
    "            # We need CSR format for fast row manipulations.\n",
    "            X = X.tocsr()\n",
    "        else:\n",
    "            # We need to convert X to a matrix, so that the indexing\n",
    "            # returns 2D objects\n",
    "            X = np.asmatrix(X)\n",
    "        n_samples = X.shape[0]\n",
    "\n",
    "        terms = np.array(list(self.vocabulary_.keys()))\n",
    "        indices = np.array(list(self.vocabulary_.values()))\n",
    "        inverse_vocabulary = terms[np.argsort(indices)]\n",
    "\n",
    "        return [inverse_vocabulary[X[i, :].nonzero()[1]].ravel()\n",
    "                for i in range(n_samples)]\n",
    "\n",
    "    def get_feature_names(self):\n",
    "        \"\"\"Array mapping from feature integer indices to feature name\"\"\"\n",
    "\n",
    "        # return self.vocabulary_.keys()\n",
    "        return [k for k, v in sorted(self.vocabulary_.iteritems(), key=lambda item: item[1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 loops, best of 3: 11.4 ms per loop\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "\n",
    "vect =  CountVectorizer(analyzer=\"char\")\n",
    "vect.fit_transform(data.crowd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 loops, best of 3: 9.17 ms per loop\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "\n",
    "vect =  CountVectorizer(analyzer=\"word\", tokenizer=lambda x: x)\n",
    "vect.fit_transform(data.crowd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 loops, best of 3: 11.8 ms per loop\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "\n",
    "vect =  CountVectorizer(analyzer=\"word\", preprocessor=lambda x: x)\n",
    "m = vect.fit_transform(data.brand_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'10005', u'1001', u'1003', u'10063', u'10123', u'10211', u'1023', u'10255', u'1029', u'1031', u'1033', u'1041', u'1045', u'1047', u'1049', u'10533', u'10625', u'1067', u'107', u'1071', u'1075', u'10773', u'1079', u'1083', u'1087', u'1089', u'1099', u'1101', u'1103', u'11055', u'11063', u'1107', u'1109', u'1119', u'11241', u'11327', u'1133', u'11367', u'1139', u'11397', u'1141', u'11467', u'11553', u'11589', u'1159', u'1163', u'1165', u'11683', u'1173', u'11741', u'1175', u'1177', u'1179', u'11859', u'1187', u'1189', u'11949', u'1197', u'11979', u'1199', u'12019', u'1203', u'1205', u'1213', u'12181', u'12195', u'12257', u'12301', u'1233', u'12365', u'1239', u'12461', u'12521', u'1253', u'1273', u'12731', u'12867', u'12869', u'12883', u'1291', u'13', u'13021', u'13145', u'1329', u'13305', u'13313', u'1353', u'13647', u'1367', u'13709', u'1383', u'13833', u'1391', u'13935', u'1395', u'1397', u'1409', u'14123', u'1417', u'1461', u'14653', u'14681', u'14701', u'1471', u'14979', u'1503', u'15031', u'1509', u'15183', u'1535', u'1557', u'1561', u'1565', u'1567', u'15719', u'1573', u'1575', u'1583', u'15845', u'15877', u'16125', u'1615', u'1649', u'16503', u'1653', u'16569', u'16579', u'16589', u'16629', u'1673', u'16747', u'1675', u'1677', u'1681', u'16817', u'16823', u'1683', u'1685', u'16865', u'16867', u'1687', u'1695', u'16997', u'17049', u'171', u'17127', u'1713', u'17139', u'17155', u'17195', u'17229', u'17237', u'1729', u'17331', u'17373', u'17389', u'17403', u'17469', u'17489', u'17519', u'1753', u'17553', u'17581', u'1761', u'17623', u'17637', u'177', u'1777', u'17853', u'1791', u'18061', u'181', u'18169', u'1825', u'1829', u'18315', u'18345', u'18427', u'18435', u'18439', u'18445', u'18531', u'18693', u'18743', u'18853', u'1897', u'19', u'19003', u'19179', u'19197', u'1931', u'19319', u'1933', u'19385', u'19637', u'19639', u'1973', u'1977', u'1981', u'19863', u'19991', u'20007', u'2005', u'2007', u'2013', u'2015', u'2017', u'20197', u'20199', u'2031', u'20393', u'20411', u'20421', u'20431', u'20447', u'2049', u'20555', u'20575', u'2067', u'20677', u'20695', u'2083', u'2085', u'2087', u'2089', u'2091', u'20919', u'2093', u'2097', u'20995', u'20997', u'21', u'2101', u'2103', u'2111', u'21127', u'21147', u'2115', u'21265', u'21267', u'21271', u'2129', u'2131', u'21313', u'21339', u'21361', u'21363', u'21417', u'21457', u'2149', u'2151', u'2161', u'2165', u'2169', u'2171', u'21821', u'2183', u'2187', u'2189', u'2197', u'22003', u'22057', u'2209', u'221', u'2217', u'2223', u'2231', u'2239', u'225', u'22519', u'2253', u'2257', u'22625', u'22701', u'22809', u'2287', u'22921', u'22989', u'2299', u'22999', u'23063', u'2311', u'23125', u'23203', u'23221', u'2327', u'233', u'23535', u'23547', u'2357', u'23611', u'23625', u'23735', u'23737', u'23825', u'23839', u'23853', u'23979', u'2405', u'2419', u'24231', u'24245', u'2431', u'24347', u'24421', u'24495', u'2451', u'2453', u'2457', u'24655', u'247', u'2471', u'24723', u'2473', u'2475', u'2477', u'249', u'24905', u'2491', u'2495', u'24961', u'24997', u'2503', u'25031', u'2505', u'2509', u'25167', u'25171', u'2519', u'2521', u'2523', u'253', u'25375', u'25455', u'2547', u'2549', u'25491', u'255', u'25521', u'2553', u'25555', u'25559', u'2569', u'257', u'2571', u'2587', u'261', u'2615', u'26159', u'26201', u'26251', u'26289', u'2635', u'26357', u'26367', u'2637', u'26375', u'2641', u'2651', u'26651', u'26693', u'26735', u'26739', u'26797', u'2687', u'269', u'2693', u'2703', u'2707', u'27095', u'2713', u'27155', u'27197', u'27221', u'2725', u'273', u'27319', u'2743', u'27449', u'27863', u'27891', u'27897', u'28033', u'28049', u'28067', u'2821', u'28233', u'2831', u'28457', u'28493', u'285', u'2857', u'2867', u'28677', u'287', u'28703', u'28707', u'28729', u'28825', u'2899', u'29069', u'2907', u'29119', u'2913', u'29207', u'293', u'29303', u'2933', u'2939', u'295', u'29529', u'29615', u'2965', u'29785', u'299', u'29953', u'29969', u'29999', u'30017', u'3025', u'30295', u'30429', u'30433', u'30437', u'30455', u'30561', u'30593', u'3063', u'307', u'30773', u'30775', u'30831', u'309', u'30903', u'30917', u'30929', u'3093', u'31003', u'3105', u'31065', u'31067', u'31071', u'3111', u'31219', u'31287', u'313', u'3131', u'31397', u'31431', u'31443', u'315', u'3155', u'3157', u'31595', u'31607', u'3161', u'31661', u'31713', u'3177', u'31831', u'319', u'32043', u'32053', u'32083', u'32385', u'3241', u'32415', u'32453', u'32537', u'32655', u'32671', u'3271', u'32757', u'3281', u'33133', u'33199', u'33285', u'33379', u'33405', u'33481', u'33487', u'335', u'33549', u'33577', u'33603', u'33723', u'33803', u'3385', u'33851', u'33879', u'33950', u'34006', u'34074', u'34088', u'341', u'3415', u'34180', u'34254', u'34264', u'34366', u'34372', u'3443', u'3445', u'34450', u'34467', u'3447', u'34481', u'34581', u'3469', u'34720', u'34811', u'3487', u'34882', u'3489', u'34949', u'35039', u'35043', u'35069', u'35083', u'35090', u'3529', u'35549', u'35644', u'35648', u'3569', u'35779', u'35866', u'359', u'3591', u'35961', u'35969', u'3599', u'35998', u'36070', u'36257', u'36264', u'36287', u'3629', u'36301', u'36345', u'36403', u'36456', u'36472', u'36538', u'36545', u'36561', u'36565', u'36577', u'36676', u'36806', u'36832', u'36890', u'369', u'36967', u'36992', u'37', u'3701', u'3703', u'37072', u'37102', u'37152', u'37160', u'37184', u'37228', u'3725', u'37253', u'37254', u'37281', u'37300', u'37327', u'3763', u'3791', u'3809', u'3825', u'38276', u'38292', u'383', u'3841', u'38508', u'3857', u'3863', u'3873', u'38981', u'39002', u'39004', u'3909', u'3913', u'3915', u'393', u'395', u'399', u'403', u'405', u'4071', u'4081', u'4107', u'411', u'4147', u'4177', u'419', u'4191', u'4209', u'421', u'4237', u'4257', u'427', u'4305', u'4309', u'443', u'4433', u'445', u'447', u'449', u'4551', u'459', u'4601', u'4607', u'465', u'4689', u'4699', u'4751', u'481', u'4845', u'4855', u'4863', u'4873', u'49', u'495', u'4957', u'5007', u'501', u'5039', u'5061', u'5075', u'5109', u'511', u'5115', u'5157', u'517', u'519', u'5229', u'5241', u'5243', u'5245', u'5257', u'527', u'5285', u'529', u'5315', u'533', u'5343', u'539', u'5395', u'5401', u'5419', u'5479', u'5485', u'55', u'553', u'5561', u'5573', u'559', u'561', u'5625', u'563', u'567', u'5683', u'5687', u'569', u'57', u'5709', u'5743', u'5759', u'5797', u'5801', u'5803', u'581', u'5813', u'5823', u'583', u'5831', u'5847', u'585', u'5851', u'5871', u'5917', u'593', u'5941', u'595', u'5967', u'5993', u'6001', u'6063', u'6071', u'623', u'6253', u'627', u'63', u'6309', u'6387', u'6449', u'6485', u'6529', u'6585', u'6589', u'6593', u'6643', u'67', u'6753', u'6763', u'6781', u'6823', u'6829', u'6849', u'6911', u'6937', u'6979', u'6989', u'7109', u'721', u'723', u'7231', u'7265', u'727', u'729', u'7299', u'733', u'7367', u'737', u'741', u'7459', u'749', u'7505', u'753', u'7531', u'7559', u'7563', u'7587', u'7589', u'759', u'763', u'765', u'7773', u'7789', u'7795', u'7797', u'783', u'7833', u'7869', u'79', u'7901', u'7923', u'7983', u'7993', u'8015', u'803', u'805', u'809', u'8105', u'811', u'8113', u'8121', u'815', u'8199', u'8203', u'823', u'8243', u'8319', u'8331', u'8363', u'8369', u'8375', u'8433', u'85', u'8585', u'8679', u'8699', u'8733', u'8909', u'891', u'893', u'895', u'897', u'901', u'9015', u'9039', u'905', u'907', u'9081', u'9095', u'9099', u'9113', u'915', u'9169', u'9181', u'9241', u'925', u'9303', u'935', u'937', u'939', u'947', u'951', u'9553', u'9563', u'9573', u'961', u'967', u'975', u'979', u'9807', u'983', u'989', u'9903', u'991', u'9913', u'993', u'9963', u'997', u'999']\n"
     ]
    }
   ],
   "source": [
    "# %%timeit\n",
    "vect =  CountVectorizer(analyzer=\"word\")\n",
    "docs = [\"123\", \"456\", \"145\"]\n",
    "m = vect.fit_transform(data.brand_id)\n",
    "# print m.toarray()\n",
    "print vect.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MyCountVectorizer(object):\n",
    "    \"\"\"\n",
    "    将文档集转为一个 token 计数矩阵，矩阵采用 scipy.sparse.coo_matrix 稀疏矩阵。.\n",
    "\n",
    "    If you do not provide an a-priori dictionary and you do not use an analyzer\n",
    "    that does some kind of feature selection then the number of features will\n",
    "    be equal to the vocabulary size found by analyzing the data.\n",
    "\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    analyzer : string, {'word', 'char'} or callable\n",
    "        Whether the feature should be made of word or character n-grams.\n",
    "        Option 'char_wb' creates character n-grams only from text inside\n",
    "        word boundaries.\n",
    "\n",
    "        If a callable is passed it is used to extract the sequence of features\n",
    "        out of the raw, unprocessed input.\n",
    "\n",
    "    preprocessor : callable or None (default)\n",
    "        Override the preprocessing (string transformation) stage while\n",
    "        preserving the tokenizing and n-grams generation steps.\n",
    "\n",
    "    tokenizer : callable or None (default)\n",
    "        Override the string tokenization step while preserving the\n",
    "        preprocessing and n-grams generation steps.\n",
    "        Only applies if ``analyzer == 'word'``.\n",
    "\n",
    "    stop_words : string {'english'}, list, or None (default)\n",
    "        If 'english', a built-in stop word list for English is used.\n",
    "\n",
    "        If a list, that list is assumed to contain stop words, all of which\n",
    "        will be removed from the resulting tokens.\n",
    "        Only applies if ``analyzer == 'word'``.\n",
    "\n",
    "        If None, no stop words will be used. max_df can be set to a value\n",
    "        in the range [0.7, 1.0) to automatically detect and filter stop\n",
    "        words based on intra corpus document frequency of terms.\n",
    "\n",
    "    lowercase : boolean, True by default\n",
    "        Convert all characters to lowercase before tokenizing.\n",
    "\n",
    "    token_pattern : string\n",
    "        Regular expression denoting what constitutes a \"token\", only used\n",
    "        if ``analyzer == 'word'``. The default regexp select tokens of 2\n",
    "        or more alphanumeric characters (punctuation is completely ignored\n",
    "        and always treated as a token separator).\n",
    "\n",
    "    vocabulary : Mapping or iterable, optional\n",
    "        Either a Mapping (e.g., a dict) where keys are terms and values are\n",
    "        indices in the feature matrix, or an iterable over terms. If not\n",
    "        given, a vocabulary is determined from the input documents. Indices\n",
    "        in the mapping should not be repeated and should not have any gap\n",
    "        between 0 and the largest index.\n",
    "\n",
    "    binary : boolean, default=False\n",
    "        If True, all non zero counts are set to 1. This is useful for discrete\n",
    "        probabilistic models that model binary events rather than integer\n",
    "        counts.\n",
    "\n",
    "    dtype : type, optional\n",
    "        Type of the matrix returned by fit_transform() or transform().\n",
    "\n",
    "    doc_type : str, {'number', 'str'}, default='str'\n",
    "\n",
    "    is_record_analyzer_res : boolean, default=False\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    vocabulary_ : dict\n",
    "        A mapping of terms to feature indices.\n",
    "\n",
    "    stop_words_ : set\n",
    "        Terms that were ignored because they either:\n",
    "\n",
    "          - occurred in too many documents (`max_df`)\n",
    "          - occurred in too few documents (`min_df`)\n",
    "          - were cut off by feature selection (`max_features`).\n",
    "\n",
    "        This is only available if no vocabulary was given.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    The ``stop_words_`` attribute can get large and increase the model size\n",
    "    when pickling. This attribute is provided only for introspection and can\n",
    "    be safely removed using delattr or set to None before pickling.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, lowercase=True, stop_words=None,\n",
    "                 vocabulary=None, binary=False,\n",
    "                 dtype=np.int8):\n",
    "        self.lowercase = lowercase\n",
    "        self.stop_words = stop_words\n",
    "        self.vocabulary = vocabulary\n",
    "        self.binary = binary\n",
    "        self.dtype = dtype\n",
    "\n",
    "    def _validate_vocabulary(self):\n",
    "        vocabulary = self.vocabulary\n",
    "        # vocabulary 不为空\n",
    "        if vocabulary is not None:\n",
    "            if isinstance(vocabulary, set):\n",
    "                vocabulary = sorted(vocabulary)\n",
    "            if not isinstance(vocabulary, Mapping):\n",
    "                vocab = {}\n",
    "                for i, t in enumerate(vocabulary):\n",
    "                    if vocab.setdefault(t, i) != i:\n",
    "                        msg = \"Duplicate term in vocabulary: %r\" % t\n",
    "                        raise ValueError(msg)\n",
    "                vocabulary = vocab\n",
    "            else:\n",
    "                indices = set(vocabulary.itervalues())\n",
    "                if len(indices) != len(vocabulary):\n",
    "                    raise ValueError(\"Vocabulary contains repeated indices.\")\n",
    "                for i in xrange(len(vocabulary)):\n",
    "                    if i not in indices:\n",
    "                        msg = (\"Vocabulary of size %d doesn't contain index \"\n",
    "                               \"%d.\" % (len(vocabulary), i))\n",
    "                        raise ValueError(msg)\n",
    "            if not vocabulary:\n",
    "                raise ValueError(\"empty vocabulary passed to fit\")\n",
    "            self.fixed_vocabulary_ = True\n",
    "            self.vocabulary_ = dict(vocabulary)\n",
    "        else:\n",
    "            self.fixed_vocabulary_ = False\n",
    "\n",
    "    def _sort_features(self, X, vocabulary):\n",
    "        \"\"\"Sort features by name\n",
    "\n",
    "        Returns a reordered matrix and modifies the vocabulary in place\n",
    "        \"\"\"\n",
    "        sorted_features = sorted(vocabulary.iteritems())\n",
    "        map_index = np.empty(len(sorted_features), dtype=np.int32)\n",
    "        for new_val, (term, old_val) in enumerate(sorted_features):\n",
    "            map_index[new_val] = old_val\n",
    "            vocabulary[term] = new_val\n",
    "        return X[:, map_index]\n",
    "\n",
    "    def _count_vocab(self, raw_documents, fixed_vocab):\n",
    "        \"\"\"Create sparse feature matrix, and vocabulary where fixed_vocab=False\n",
    "        \"\"\"\n",
    "        if fixed_vocab:\n",
    "            vocabulary = self.vocabulary_\n",
    "        else:\n",
    "            # Add a new value when a new vocabulary item is seen\n",
    "            vocabulary = defaultdict()\n",
    "            vocabulary.default_factory = vocabulary.__len__\n",
    "\n",
    "        j_indices = []\n",
    "        indptr = _make_int_array()\n",
    "        values = _make_int_array()\n",
    "        indptr.append(0)\n",
    "\n",
    "        for doc in raw_documents:\n",
    "            if self.stop_words:\n",
    "                if doc in self.stop_words:\n",
    "                    indptr.append(len(j_indices))\n",
    "                    continue\n",
    "            feature_counter = {}\n",
    "            try:\n",
    "                feature_idx = vocabulary[doc]\n",
    "                if feature_idx not in feature_counter:\n",
    "                    feature_counter[feature_idx] = 1\n",
    "                else:\n",
    "                    feature_counter[feature_idx] += 1\n",
    "            except KeyError:\n",
    "                # 使用自定义词库表时，如果文档中包含的词汇[特征]不在自定义词库表中，\n",
    "                # 则不再将该词汇[特征]加入到词库表\n",
    "                # Ignore out-of-vocabulary items for fixed_vocab=True\n",
    "                continue\n",
    "\n",
    "            j_indices.extend(feature_counter.keys())\n",
    "            values.extend(feature_counter.values())\n",
    "            indptr.append(len(j_indices))\n",
    "\n",
    "        # 自动生成[学习]词库表的情况\n",
    "        if not fixed_vocab:\n",
    "            # disable defaultdict behaviour\n",
    "            vocabulary = dict(vocabulary)\n",
    "            if not vocabulary:\n",
    "                raise ValueError(\"empty vocabulary; perhaps the documents only\"\n",
    "                                 \" contain stop words\")\n",
    "\n",
    "        j_indices = np.asarray(j_indices, dtype=np.intc)\n",
    "        indptr = np.frombuffer(indptr, dtype=np.intc)\n",
    "        values = frombuffer_empty(values, dtype=np.intc)\n",
    "\n",
    "        X = sp.csr_matrix((values, j_indices, indptr),\n",
    "                          shape=(len(indptr) - 1, len(vocabulary)),\n",
    "                          dtype=self.dtype)\n",
    "        #\n",
    "        X.sort_indices()\n",
    "        return vocabulary, X\n",
    "\n",
    "    def fit_transform(self, raw_documents):\n",
    "        \"\"\"Learn the vocabulary dictionary and return term-document matrix.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        raw_documents : iterable\n",
    "            An iterable which yields either str, unicode or file objects.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        X : array, [n_samples, n_features]\n",
    "            Document-term matrix.\n",
    "        \"\"\"\n",
    "        if isinstance(raw_documents, str):\n",
    "            raise ValueError(\n",
    "                \"Iterable over raw text documents expected, \"\n",
    "                \"string object received.\")\n",
    "\n",
    "        # 校验vocabulary，如果之前提供了自定义vocabulary，则将self.fixed_vocabulary_设为True，否则False\n",
    "        self._validate_vocabulary()\n",
    "\n",
    "        vocabulary, X = self._count_vocab(raw_documents,\n",
    "                                          self.fixed_vocabulary_)\n",
    "\n",
    "        if self.binary:\n",
    "            X.data.fill(1)\n",
    "\n",
    "        # 自动生成[学习]词库表的情况\n",
    "        if not self.fixed_vocabulary_:\n",
    "            X = self._sort_features(X, vocabulary)\n",
    "            self.vocabulary_ = vocabulary\n",
    "        return X\n",
    "\n",
    "    def inverse_transform(self, X):\n",
    "        \"\"\"Return terms per document with nonzero entries in X.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array, sparse matrix}, shape = [n_samples, n_features]\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        X_inv : list of arrays, len = n_samples\n",
    "            List of arrays of terms.\n",
    "        \"\"\"\n",
    "\n",
    "        if sp.issparse(X):\n",
    "            # We need CSR format for fast row manipulations.\n",
    "            X = X.tocsr()\n",
    "        else:\n",
    "            # We need to convert X to a matrix, so that the indexing\n",
    "            # returns 2D objects\n",
    "            X = np.asmatrix(X)\n",
    "        n_samples = X.shape[0]\n",
    "\n",
    "        terms = np.array(list(self.vocabulary_.keys()))\n",
    "        indices = np.array(list(self.vocabulary_.values()))\n",
    "        inverse_vocabulary = terms[np.argsort(indices)]\n",
    "\n",
    "        return [inverse_vocabulary[X[i, :].nonzero()[1]].ravel()\n",
    "                for i in range(n_samples)]\n",
    "\n",
    "    def get_feature_names(self):\n",
    "        \"\"\"Array mapping from feature integer indices to feature name\"\"\"\n",
    "\n",
    "        # return self.vocabulary_.keys()\n",
    "        return [k for k, v in sorted(self.vocabulary_.iteritems(), key=lambda item: item[1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d = data.brand_id.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 0 0]\n",
      " [0 0 0 1]\n",
      " [0 0 1 0]\n",
      " [1 0 0 0]]\n",
      "['0', '123', '145', '456']\n"
     ]
    }
   ],
   "source": [
    "# %%timeit\n",
    "vect = MyCountVectorizer(stop_words=['0'])\n",
    "docs = [\"123\", \"456\", \"145\", \"0\"]\n",
    "m = vect.fit_transform(docs)\n",
    "print m.toarray()\n",
    "print vect.get_feature_names()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 0]\n",
      " [0 0 1]\n",
      " [0 1 0]]\n",
      "[u'123', u'145', u'456']\n"
     ]
    }
   ],
   "source": [
    "docs = [\"123\", \"456\", \"145\"]\n",
    "vect =  CountVectorizer(analyzer=\"word\", preprocessor=lambda x: x)\n",
    "m = vect.fit_transform(docs)\n",
    "print m.toarray()\n",
    "print vect.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 1 1 1 1 0 0]\n",
      " [1 1 1 1 1 1 1 1]\n",
      " [0 0 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1]]\n",
      "达, 斯登, 速干, t, 恤, 男, 女, 短袖\n"
     ]
    }
   ],
   "source": [
    "docs = [\"达斯登 速干T恤男\",\n",
    "\"达斯登 速干T恤男 女短袖\",\n",
    "\"速干T恤男 女短袖\",\n",
    "\"达斯登 速干T恤男 女短袖\"\n",
    "]\n",
    "m1 = Vectorizer1(doc_type=\"str\", analyzer=\"word\", stop_words=[\" \"],\n",
    "                    preprocessor=None, tokenizer=jieba.cut, is_record_analyzer_res=True)\n",
    "print m1.fit_transform(docs).toarray()\n",
    "print \", \".join(m1.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1]\n",
      " [1 0]\n",
      " [0 0]]\n",
      "女, 男\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0, 1],\n",
       "       [1, 0],\n",
       "       [0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m2 = Vectorizer2(analyzer=\"word\", stop_words=[\" \"], preprocessor=None, tokenizer=jieba.cut)\n",
    "print m2.fit_transform(docs).toarray()\n",
    "print \", \".join(m2.get_feature_names())\n",
    "m = m2.fit_transform(docs)\n",
    "m.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "男\n"
     ]
    }
   ],
   "source": [
    "d = m2.inverse_transform([0,1])\n",
    "d = map(lambda x: \", \".join(x), d)\n",
    "print \"\\n\".join(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                蓝迪,智慧,乐园,儿童,磁性,彩色,绘画板\n",
       "1                        reen,润膏,蓝色,燕窝,洗发水,250ml,*,3,件\n",
       "2         tcl,kfrd,-,35gw,/,ew13bpa,1.5,匹,变频,冷暖,壁挂式,空调\n",
       "3    海信,（,hisense,）,bcd,-,321wt,/,q,321,升,风冷,无霜,节能,...\n",
       "4                            triumph,黛安芬,e002208,女士,文胸\n",
       "dtype: object"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series([\",\".join(t) for t in vect_word.documents_analyzer_res]).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1]\n",
      " [1 0]\n",
      " [0 0]]\n",
      "女, 男\n"
     ]
    }
   ],
   "source": [
    "docs = [\"男\",\n",
    "\"女\",\n",
    "\"\"\n",
    "]\n",
    "m2 = Vectorizer2(analyzer=\"word\", stop_words=[\" \"], preprocessor=None, tokenizer=lambda x: x)\n",
    "print m2.fit_transform(docs).toarray()\n",
    "print \", \".join(m2.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import array\n",
    "from collections import defaultdict, Mapping\n",
    "\n",
    "import re\n",
    "# import jieba\n",
    "# jieba.initialize()\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "\n",
    "from numpy import frombuffer as frombuffer_empty\n",
    "\n",
    "def _make_int_array():\n",
    "    \"\"\"Construct an array.array of a type suitable for scipy.sparse indices.\"\"\"\n",
    "    return array.array(str(\"i\"))\n",
    "\n",
    "\n",
    "class MyCountVectorizer(object):\n",
    "    \"\"\"\n",
    "    将文档集转为一个 token 计数矩阵，矩阵采用 scipy.sparse.coo_matrix 稀疏矩阵。.\n",
    "\n",
    "    If you do not provide an a-priori dictionary and you do not use an analyzer\n",
    "    that does some kind of feature selection then the number of features will\n",
    "    be equal to the vocabulary size found by analyzing the data.\n",
    "\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    analyzer : string, {'word', 'char'} or callable\n",
    "        Whether the feature should be made of word or character n-grams.\n",
    "        Option 'char_wb' creates character n-grams only from text inside\n",
    "        word boundaries.\n",
    "\n",
    "        If a callable is passed it is used to extract the sequence of features\n",
    "        out of the raw, unprocessed input.\n",
    "\n",
    "    preprocessor : callable or None (default)\n",
    "        Override the preprocessing (string transformation) stage while\n",
    "        preserving the tokenizing and n-grams generation steps.\n",
    "\n",
    "    tokenizer : callable or None (default)\n",
    "        Override the string tokenization step while preserving the\n",
    "        preprocessing and n-grams generation steps.\n",
    "        Only applies if ``analyzer == 'word'``.\n",
    "\n",
    "    stop_words : string {'english'}, list, or None (default)\n",
    "        If 'english', a built-in stop word list for English is used.\n",
    "\n",
    "        If a list, that list is assumed to contain stop words, all of which\n",
    "        will be removed from the resulting tokens.\n",
    "        Only applies if ``analyzer == 'word'``.\n",
    "\n",
    "        If None, no stop words will be used. max_df can be set to a value\n",
    "        in the range [0.7, 1.0) to automatically detect and filter stop\n",
    "        words based on intra corpus document frequency of terms.\n",
    "\n",
    "    lowercase : boolean, True by default\n",
    "        Convert all characters to lowercase before tokenizing.\n",
    "\n",
    "    token_pattern : string\n",
    "        Regular expression denoting what constitutes a \"token\", only used\n",
    "        if ``analyzer == 'word'``. The default regexp select tokens of 2\n",
    "        or more alphanumeric characters (punctuation is completely ignored\n",
    "        and always treated as a token separator).\n",
    "\n",
    "    vocabulary : Mapping or iterable, optional\n",
    "        Either a Mapping (e.g., a dict) where keys are terms and values are\n",
    "        indices in the feature matrix, or an iterable over terms. If not\n",
    "        given, a vocabulary is determined from the input documents. Indices\n",
    "        in the mapping should not be repeated and should not have any gap\n",
    "        between 0 and the largest index.\n",
    "\n",
    "    binary : boolean, default=False\n",
    "        If True, all non zero counts are set to 1. This is useful for discrete\n",
    "        probabilistic models that model binary events rather than integer\n",
    "        counts.\n",
    "\n",
    "    dtype : type, optional\n",
    "        Type of the matrix returned by fit_transform() or transform().\n",
    "\n",
    "    doc_type : str, {'number', 'str'}, default='str'\n",
    "\n",
    "    is_record_analyzer_res : boolean, default=False\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    vocabulary_ : dict\n",
    "        A mapping of terms to feature indices.\n",
    "\n",
    "    stop_words_ : set\n",
    "        Terms that were ignored because they either:\n",
    "\n",
    "          - occurred in too many documents (`max_df`)\n",
    "          - occurred in too few documents (`min_df`)\n",
    "          - were cut off by feature selection (`max_features`).\n",
    "\n",
    "        This is only available if no vocabulary was given.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    The ``stop_words_`` attribute can get large and increase the model size\n",
    "    when pickling. This attribute is provided only for introspection and can\n",
    "    be safely removed using delattr or set to None before pickling.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, lowercase=True, stop_words=None,\n",
    "                 vocabulary=None, binary=False,\n",
    "                 dtype=np.int8):\n",
    "        self.lowercase = lowercase\n",
    "        self.stop_words = stop_words\n",
    "        self.vocabulary = vocabulary\n",
    "        self.binary = binary\n",
    "        self.dtype = dtype\n",
    "\n",
    "    def _validate_vocabulary(self):\n",
    "        vocabulary = self.vocabulary\n",
    "        # vocabulary 不为空\n",
    "        if vocabulary is not None:\n",
    "            if isinstance(vocabulary, set):\n",
    "                vocabulary = sorted(vocabulary)\n",
    "            if not isinstance(vocabulary, Mapping):\n",
    "                vocab = {}\n",
    "                for i, t in enumerate(vocabulary):\n",
    "                    if vocab.setdefault(t, i) != i:\n",
    "                        msg = \"Duplicate term in vocabulary: %r\" % t\n",
    "                        raise ValueError(msg)\n",
    "                vocabulary = vocab\n",
    "            else:\n",
    "                indices = set(vocabulary.itervalues())\n",
    "                if len(indices) != len(vocabulary):\n",
    "                    raise ValueError(\"Vocabulary contains repeated indices.\")\n",
    "                for i in xrange(len(vocabulary)):\n",
    "                    if i not in indices:\n",
    "                        msg = (\"Vocabulary of size %d doesn't contain index \"\n",
    "                               \"%d.\" % (len(vocabulary), i))\n",
    "                        raise ValueError(msg)\n",
    "            if not vocabulary:\n",
    "                raise ValueError(\"empty vocabulary passed to fit\")\n",
    "            self.fixed_vocabulary_ = True\n",
    "            self.vocabulary_ = dict(vocabulary)\n",
    "        else:\n",
    "            self.fixed_vocabulary_ = False\n",
    "\n",
    "    def _sort_features(self, X, vocabulary):\n",
    "        \"\"\"Sort features by name\n",
    "\n",
    "        Returns a reordered matrix and modifies the vocabulary in place\n",
    "        \"\"\"\n",
    "        sorted_features = sorted(vocabulary.iteritems())\n",
    "        map_index = np.empty(len(sorted_features), dtype=np.int32)\n",
    "        for new_val, (term, old_val) in enumerate(sorted_features):\n",
    "            map_index[new_val] = old_val\n",
    "            vocabulary[term] = new_val\n",
    "        return X[:, map_index]\n",
    "\n",
    "    def _count_vocab(self, raw_documents, fixed_vocab):\n",
    "        \"\"\"Create sparse feature matrix, and vocabulary where fixed_vocab=False\n",
    "        \"\"\"\n",
    "        if fixed_vocab:\n",
    "            vocabulary = self.vocabulary_\n",
    "        else:\n",
    "            # Add a new value when a new vocabulary item is seen\n",
    "            vocabulary = defaultdict()\n",
    "            vocabulary.default_factory = vocabulary.__len__\n",
    "\n",
    "        j_indices = []\n",
    "        indptr = _make_int_array()\n",
    "        values = _make_int_array()\n",
    "        indptr.append(0)\n",
    "\n",
    "        for doc in raw_documents:\n",
    "            if doc in self.stop_words:\n",
    "                indptr.append(len(j_indices))\n",
    "                continue\n",
    "            feature_counter = {}\n",
    "            try:\n",
    "                feature_idx = vocabulary[doc]\n",
    "                if feature_idx not in feature_counter:\n",
    "                    feature_counter[feature_idx] = 1\n",
    "                else:\n",
    "                    feature_counter[feature_idx] += 1\n",
    "            except KeyError:\n",
    "                # 使用自定义词库表时，如果文档中包含的词汇[特征]不在自定义词库表中，\n",
    "                # 则不再将该词汇[特征]加入到词库表\n",
    "                # Ignore out-of-vocabulary items for fixed_vocab=True\n",
    "                continue\n",
    "\n",
    "            j_indices.extend(feature_counter.keys())\n",
    "            values.extend(feature_counter.values())\n",
    "            indptr.append(len(j_indices))\n",
    "\n",
    "        # 自动生成[学习]词库表的情况\n",
    "        if not fixed_vocab:\n",
    "            # disable defaultdict behaviour\n",
    "            vocabulary = dict(vocabulary)\n",
    "            if not vocabulary:\n",
    "                raise ValueError(\"empty vocabulary; perhaps the documents only\"\n",
    "                                 \" contain stop words\")\n",
    "\n",
    "        j_indices = np.asarray(j_indices, dtype=np.intc)\n",
    "        indptr = np.frombuffer(indptr, dtype=np.intc)\n",
    "        values = frombuffer_empty(values, dtype=np.intc)\n",
    "\n",
    "        X = sp.csr_matrix((values, j_indices, indptr),\n",
    "                          shape=(len(indptr) - 1, len(vocabulary)),\n",
    "                          dtype=self.dtype)\n",
    "        #\n",
    "        X.sort_indices()\n",
    "        return vocabulary, X\n",
    "\n",
    "    def fit_transform(self, raw_documents):\n",
    "        \"\"\"Learn the vocabulary dictionary and return term-document matrix.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        raw_documents : iterable\n",
    "            An iterable which yields either str, unicode or file objects.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        X : array, [n_samples, n_features]\n",
    "            Document-term matrix.\n",
    "        \"\"\"\n",
    "        if isinstance(raw_documents, str):\n",
    "            raise ValueError(\n",
    "                \"Iterable over raw text documents expected, \"\n",
    "                \"string object received.\")\n",
    "\n",
    "        # 校验vocabulary，如果之前提供了自定义vocabulary，则将self.fixed_vocabulary_设为True，否则False\n",
    "        self._validate_vocabulary()\n",
    "\n",
    "        vocabulary, X = self._count_vocab(raw_documents,\n",
    "                                          self.fixed_vocabulary_)\n",
    "\n",
    "        if self.binary:\n",
    "            X.data.fill(1)\n",
    "\n",
    "        # 自动生成[学习]词库表的情况\n",
    "        if not self.fixed_vocabulary_:\n",
    "            X = self._sort_features(X, vocabulary)\n",
    "            self.vocabulary_ = vocabulary\n",
    "        return X\n",
    "\n",
    "    def inverse_transform(self, X):\n",
    "        \"\"\"Return terms per document with nonzero entries in X.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array, sparse matrix}, shape = [n_samples, n_features]\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        X_inv : list of arrays, len = n_samples\n",
    "            List of arrays of terms.\n",
    "        \"\"\"\n",
    "\n",
    "        if sp.issparse(X):\n",
    "            # We need CSR format for fast row manipulations.\n",
    "            X = X.tocsr()\n",
    "        else:\n",
    "            # We need to convert X to a matrix, so that the indexing\n",
    "            # returns 2D objects\n",
    "            X = np.asmatrix(X)\n",
    "        n_samples = X.shape[0]\n",
    "\n",
    "        terms = np.array(list(self.vocabulary_.keys()))\n",
    "        indices = np.array(list(self.vocabulary_.values()))\n",
    "        inverse_vocabulary = terms[np.argsort(indices)]\n",
    "\n",
    "        return [inverse_vocabulary[X[i, :].nonzero()[1]].ravel()\n",
    "                for i in range(n_samples)]\n",
    "\n",
    "    def get_feature_names(self):\n",
    "        \"\"\"Array mapping from feature integer indices to feature name\"\"\"\n",
    "\n",
    "        # return self.vocabulary_.keys()\n",
    "        return [k for k, v in sorted(self.vocabulary_.iteritems(), key=lambda item: item[1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"./off_line_file/article_data_v5_2017-03-01-20-56.txt\", sep=\"\\t\", encoding=\"utf8\")\n",
    "\n",
    "# model_main = ModelMain(data, features_with_weight_1,\n",
    "#                        features_with_weight_2, src_article_num=None,\n",
    "#                        rec_article_num=3, title_fc_extra_weight=None,\n",
    "#                        rec_format=\"list\", ndigits=2)\n",
    "# r1 = model_main.run_similary()\n",
    "pp = PreProcess()\n",
    "data = pp(data)\n",
    "model = SimilaryModel(data, features_with_weight_1,\n",
    "                      features_with_weight_2, src_article_num=None,\n",
    "                      rec_article_num=3, title_fc_extra_weight=None,\n",
    "                      ndigits=2)\n",
    "res_csr_matrix = model.calculate_similary()\n",
    "res = model.map_articles(res_csr_matrix, res_format=\"list\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# coding=utf-8\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "from model_title_fenci_weight_up.model import ModelMain, features_with_weight_1, features_with_weight_2, SimilaryModel\n",
    "from model_title_fenci_weight_up.preprocessing import PreProcess\n",
    "\n",
    "def construct_frame(res_list, res_cols):\n",
    "    return pd.DataFrame(data=res_list, columns=res_cols)\n",
    "\n",
    "\n",
    "def add_article_attrs(dataframe, associate_attrs, flag=\"src\"):\n",
    "    left_on_key = \"src_article\" if flag==\"src\" else \"rec_article\"\n",
    "    src_col_list = dataframe.columns.tolist()\n",
    "    src_col_list.extend(associate_attrs)\n",
    "    dataframe = dataframe.merge(data, left_on=left_on_key, right_on=\"article_id\")\n",
    "    cols_map = dict(zip(associate_attrs, map(lambda s: flag + \"_\" + s, associate_attrs)))\n",
    "    dataframe.rename_axis(cols_map, axis=1, inplace=True)\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "res_cols = [u\"src_article\", u\"rec_article\", u\"score_sum\",\n",
    "        u\"score_pro\", u\"score_level1\", u\"score_level2\",\n",
    "        u\"score_level3\", u\"score_level4\", u\"score_brand\",\n",
    "        u\"score_title_fc\", u\"score_sex\", u\"score_crowd\"]\n",
    "associate_cols = [\"pro_id\", \"level_1\", \"level_2\", \"level_3\", \"level_4\", \"brand\", \"title\", \"title_fc\", \"pubdate\"]\n",
    "\n",
    "recommend = construct_frame(res, res_cols)\n",
    "recommend = add_article_attrs(recommend, associate_cols, flag=\"src\")\n",
    "print recommend.columns\n",
    "recommend = add_article_attrs(recommend, associate_cols, flag=\"rec\")\n",
    "# recommend.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sorted_cols = [u'src_article', u'rec_article', u'src_title', u'rec_title', u'score_sum', u'score_pro', u'src_pro_id',  u'rec_pro_id',\n",
    " u'score_level4', u'src_level_4', u'rec_level_4', u'score_level3', u'src_level_3', u'rec_level_3',\n",
    " u'score_level2', u'src_level_2', u'rec_level_2', u'score_level1', u'src_level_1', u'rec_level_1',\n",
    " u'score_brand', u'src_brand', u'rec_brand', u'score_title_fc', u'src_title_fc', u'rec_title_fc', u'score_sex', u'score_crowd']\n",
    "recommend = recommend[sorted_cols]\n",
    "recommend = recommend.sort_values(by=[\"src_article\",\"score_sum\"], ascending=False)\n",
    "recommend.head()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
