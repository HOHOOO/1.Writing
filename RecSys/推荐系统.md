
一.目标现在已经进入大数据时代, 数据是无缝连接网络世界与物理世界的DNA。发现数据DNA、重组数据DNA是人类不断认识、探索、实践大数据的持续过程。大数据分析可以有效地促进营销,个性化医疗治病，帮助学生提高成绩,利于老师提高教学水平,还可以用于教学，许多产品可以用到大数据技术，如量化分析金融产品等。必须加强大数据技术的研究并实际应用.这里对目前最流行和最实用的用户画像技术进行讲解，并分析大数据分析的常用算法。原文有大量图片，完整文档见百度http://pan.baidu.com/s/1eQKwT38

二.用户画像1. 定义用户画像指根据用户的特性和行为数据建立标签，准确描述用户的特点，支持对用户的精准营销等活动。用户画像，即用户信息标签化，就是企业通过收集与分析消费者社会属性、生活习惯、消费行为等主要信息的数据之后，完美地抽象出一个用户的商业全貌作是企业应用大数据技术的基本方式。用户画像为企业提供了足够的信息基础，能够帮助企业快速找到精准用户群体以及用户需求等更为广泛的反馈信息。

大数据用户画像其实就是对现实用户做的一个数学模型，在整个数学模型中，其核心是，怎么描述业务知识体系，而这个业务知识体系就是本体论，本体论很复杂，我们找到了一个特别朴素的实现，就是标签。建好模型以后，要在业务的实践中去检验，并且不断完善，不断丰富这个模型，来达到利用比特流对人越来越精确的理解。用户画像不是一个数学游戏，不是一个技术问题，实际上是一个业务问题。因为最核心的是你去如何理解用户，了解你的用户。它是技术与业务最佳的结合点，也是一个现实跟数据的最佳实践。

一个画像样例：基于他这个人可以知道他所在的城市是在北京，男性，公司在百分点，喜欢的品类是男鞋、运动鞋，喜欢的品牌有耐克、阿迪达斯等等。每一个标签都有一个权重值。可以看到，耐克的权重值比阿迪达斯更高一些。



2用户画像方法用户画像的焦点工作就是为用户打“标签”，而一个标签通常是人为规定的高度精炼的特征标识，如年龄、性别、地域、用户偏好等，最后将用户的所有标签综合来看，就可以勾勒出该用户的立体“画像”了。

具体来讲，当为用户画像时，需要以下四个阶段：

1.战略解读：企业选择构建用户画像平台，可以实现不同的战略目的，如提升产品服务质量、精准营销等。根据战略目的的不同，用户画像的构建也有所区别。因此首先需要明确用户画像平台的战略意义、平台建设目标和效果预期，进而有针对性的开展实施工作。

2.建模体系：对用户画像进行数据建模，结合客户实际的需求，找出相关的数据实体，以数据实体为中心规约数据维度类型和关联关系，形成符合客户实际情况的建模体系。

3.维度分解：以用户、商品、渠道三类数据实体为中心，进行数据维度分解和列举。根据相关性原则，选取和战略目的相关的数据维度，避免产生过多无用数据干扰分析过程。

4.应用流程：针对不同角色人员的需求（如市场、销售、研发等），设计各角色人员在用户画像工具中的使用功能和应用/操作流程。

用户画像平台的战略意义：

1.完善产品运营，提升用户体验：改变以往闭门造车的生产模式，通过事先调研用户需求，设计制造更适合用户的产品，提升用户体验。

2.对外服务，提升盈利：根据产品特点，找到目标用户，在用户偏好的渠道上与其交互，促成购买，实现精准运营和营销。

如何搭建用户画像平台？

公司应搭建一个用户画像平台，将本身拥有大量用户数据的数据平台和可视化数据工具平台连接起来，根据不同的用户交互场景，应用挖掘数据平台的价值，让研发生产，用户研究，市场营销等人员能够根据需要，随时自主地分析不同产品用户特征，快速洞察用户需求。该平台需要回答的核心问题是：

用户是谁?

用户需求是什么?

用户在哪里？

用户画像建模体系

完善的用户画像平台需要考虑周全的模型体系。通常来讲，构建用户画像平台所需的数据分成用户、商品、渠道三类实体。

1.用户：数据维度包括自然特征、兴趣特征、社会特征、消费特征。从数据特点上看，又可分为基本属性和衍生标签，基本属性包括年龄、性别、地域、收入等客观事实数据，衍生标签属于基本属性为依据，通过模型规则生成的附加判断数据。

2.商品：数据维度包括商品定位和商品属性。商品属性即商品的功能、颜色、能耗、价格等事实数据，商品定位即商品的风格和定位人群，需要和用户标签进行匹配。

3.渠道：渠道分为信息渠道和购买渠道。用户在信息渠道上获得资讯，在购买渠道上进行商品采购。不同类型的用户对渠道有不同的偏好，精准的选择对应的渠道才能提高效率和收益。

用户画像数据维度

针对每一类数据实体，进一步分解可落地的数据维度，形成字段集。

1.用户数据：

用户画像使用场景

场景一，按需设计：改变原有的先设计、再销售的传统模式，在研发新产品前，先基于产品期望定位，在用户画像平台中分析该用户群体的偏好，有针对性的设计产品，从而改变原先新产品高失败率的窘境，增强销售表现。比如，某公司想研发一款智能手表，面向28-35岁的年轻男性，通过在平台中进行分析，发现材质=“金属”、风格=“硬朗”、颜色=“黑色”/"深灰色"、价格区间=“中等”的偏好比重最大，那么就给新产品的设计提供了非常客观有效的决策依据。

场景二，精准营销：针对已有产品，寻找所偏好的精准人群分类，以及这些人群在信息渠道和购买渠道上的分布比例，来决定广告投放和活动开展的位置、内容等，实现精准营销。

用户画像平台技术方案

系统架构

从数据源到最终展现分成如下几层：

1.数据源：包括来自各个业务系统和媒介的分析数据源，其载体包括数据库、文件、大数据平台等。

2.数据建模：根据用户画像建模体系，配置数据模型。

3.数据集市：每个数据集市是基于一个主题做好轻量建模的细节数据，数据按照列存储的方式，被高效压缩，打好标签，存储在磁盘中。当需要计算时，采用内存计算来进行数据计算，并且每台机器节点会同时计算，最终会将结果送往可视化分析层做展现。

4.可视化分析：采用永洪敏捷可视化分析作为前端交互组件。无论是业务用户还是IT开发人员都可以通过主流浏览器来访问可视化分析系统，用户还可通过移动终端来访问系统。可视化分析系统提供系统监控，权限多级管理，多维数据分析，等等功能，还支持自服务式报表设计和数据分析。

可视化分析：敏捷可视化分析

3用户画像架构百分点的画像标签体系包括：人口属性、上网特征、营销特征、内容偏好、兴趣偏好等。



以手机商品属性为例，包括品牌、品类、型号、上市时间、价格、颜色、网络、操作系统、分辨率、屏幕尺寸等等。



标签管理体系具有如下特性。



有多种标识方式对用户进行识别，这就像社会生活中的***号码一样，只不过换成了网络空间的手机号、Cookie、IMEI、Email、微博、微信账号等，在处理过程中，这些信息都是加密的，机器知道但人不知道。

百分点用户画像逻辑架构如下图所示，通过对电商、社区、移动应用、微博、微信等多种类别的数据源进行采集，然后对用户进行画像，最终在个性化推荐、用户洞察、精准营销等方面进行应用。百分点的数据源多且庞大，服务的客户超过了1500多家，覆盖行业超过了40多个。举例来说，一个网民，他在访问一个电商A，同时又访问了一个电商B，这两个电商本身的知识体系是不一样的。比如说这个用户他访问一双鞋，他在电商A上的品类可能是鞋-男鞋-运动鞋，在网站B上可能是运动-户外-男鞋，品类描述可能是不一样的。所以百分点打造了这么一个系统，叫商品画像系统。通过这个系统，所有的标签就有了一个标签规划，之后就可以去构建这个用户在全网的用户画像标签。用户画像只是一个起点，而不是一个结束。基于此，还可以打造一系列的服务，比如精准营销、个性化推荐等。



下图是用户画像的技术架构图。我们可以看到总共分为五层：第一是数据源；第二层是数据采集服务，百分点有一堆数据采集服务，包括我们的数据探头，能够对用户的行为进行一个实时采集；第三层是数据预处理，主要是结构化；第四层是商品画像，这一块都是我们的用户画像服务。我们可以看到用户画像是分两大块，实时处理更偏重于预测用户画像的需求，离线处理更偏重于用户的长期偏好；第五是统一的数据接口，还有就是集群，上面可以接入各种各样的应用。



下图是用户标签产出流程示例。



用户在互联网上的行为主要分为电商类、社交类和媒体类。每种行为差异很大，电商类行为包括浏览、搜索、添加购物车、收藏、支付等，而社交类则是点赞、转发、评论等。



接着下一步需要对页面标签进行抽取，在做这件事情之前需要训练模型，首先准备训练数据，通过标注和规则生成，再就是对于序列集做一个序列化处理。首先会得到一个弱模型，最终得到一个强模型，然后把自己的参数都保留下来。这个时候我们会加一个决策，如果说效果不太好的话，我们会进行下一轮的优化。当这个模型设置之后，我们就可以去做预测了。我们的预测总共分为四大块，包括输入、输入预处理、预测和产出。也就是说用户这个标签已经有了，这个标签对用户的信誉度是1还是0？这个时候就到了用户行为建模。用户行为建模的背后思想主要可以认为有两大块，成本越高行为权重越高，下单就比浏览更高一些，时间越近的行为权重越高，比如我今天看了一个手机，一定比我一周前看了一次电脑权重要高一些。我们可以按场景去分，首先是产生需求，再就是决策，然后是结束，百分点基于业务考虑，实行标签权重积累的机制。



这是我们的客户某航空公司的案例，项目目的是挖掘高价值旅客，希望通过分析旅客出行偏好优化运力资源。最终百分点帮他构建了5个标签大类，75个标签小类，数万个小标签，以下是当时的一些效果截图。



刚才讲的都是百分点已经做的事情，但是百分点做得还远远不够。接下来可能会在四大方面深入思考和实践：一是不同的场景，也就是说用户在家里和在办公环境下代表的偏好是不一样的；二是用户心理学特征，比如当一个用户看一件女装的时候，她这个时候是无聊去逛还是有目的的逛，反映在标签权重上是不一样的；三是让用户主动反馈反感点，我们强调了许多，一般都是在强调用户喜欢什么，但是用户不喜欢什么，我们做得还不够，我们应该让用户主动告诉我们他不喜欢什么，比如他不喜欢吃葱，他不喜欢吃羊肉串，这样我们预测的时候会准得多；四是用户的兴趣转移快速捕获，一开始我们使用的是一个半衰期的，而且按频率细分，我们是否可以按人去分？比如按访次去分？比如针对品类手机这个标签，对于手机发烧友，可能过了一年他依然会对手机比较感兴趣，但是对于像我这种，只有想购买的时候才去看，可能我两天不看，就表示这个兴趣已经衰减为零了。



4基于用户画像的精准营销通过建立模型对用户的数据建标签，实现用户画像，再根据推荐算法实现精准营销，模型要考虑精度和稳定度，通过交叉验证选择最优的梦想,并根据新的数据进行充分的修改、完善。

推荐系统的业务架构如下：

第一层是推荐业务活动层，把推荐结果展示给用户。

第二层是推荐算法层，包括用户画像推荐、情景推荐等。

第三层是索引层，对产品、用户数据建立索引，提高查询速度。

第四层是数据层，存储用户、产品及推荐基础数据如推荐模型。



1场景引擎：个性化的核心，判断用户处于哪个购物环节，有什么样的购物目标；

2规则引擎：业务的核心，结合用户、场景、算法输出数据和业务KPI，决定为用户推荐哪些内容；

3算法引擎：计算用户之间的相似度、商品之间的相似度、用户对商品的评分、用户分群、热门排行……，采用用户协同相关和商品协同相关算法，根据用户画像的数据进行推荐。

4展示引擎：将推荐内容以最佳的展示方式呈现在用户面前。

推荐主要是深挖用户潜在的购物兴趣或学习兴趣，在合适的时间推荐合适的商品，做到有的放矢，从而实现精准营销，提升转化率和效益。推荐的方法是，根据用户的短期兴趣采用用户意图引擎推荐商品，根据用户的长期兴趣采用用户画像引擎推荐商品，根据用户的潜在兴趣采用千人千面引擎基于用户协同相关算法推荐商品；根据合适的场景采用情景推荐引擎推荐商品，根据用户的购物周期采用反向推荐引擎推荐商品，根据最近热点和用户行为推荐商品。推荐可以用到许多方面，如：

根据用户和购买场景使用storm进行实时计算，给出推荐结果；对大量样本数据使用spark进行离线机器学习计算，产生模型，用于用户画像权重确定和实时计算。大规模批处理使用hadoop的mapReduce计算。对用户的搜索也可以使用用户画像和商品画像进行结果展示。用户的行为数据在变化，商品的信息也在变化，用户画像和商品画像需要定时进行修改，比如半个月或一个月。在hbase中对商品标签和用户标签等建有表保存数据，并根据这些数据用机器学习训练算法模型，模型结果保存在hbase，在具体推荐的时候取近一个月的数据带入模型进行计算，多种推荐结果根据规则计算后的到最佳的推荐结果，再用展示引擎显示给用户。计算的中间结果保存在hbase中。用户购买之后，商品和用户的标签要及时更新。通过用户画像精准营销可以把转化率提升30-500%。

用户画像的推荐算法：

用户画像的算法及其他算法都需要不断优化。

商品、用户的画像标签权重的计算公式如下：



主题标签的权重计算公式：

根据计算公式算出每个标签的weight。

5.推荐算法构建方法1.计算权重系数ɑ

画像权重需要考虑时间、地点即网址、网页内容、事件即用户活动,如购买商品，下面是具体例子。
什么时间：时间包括两个重要信息，时间戳+时间长度。时间戳，为了标识用户行为的时间点，如，1395121950（精度到秒），1395121950.083612（精度到微秒），通常采用精度到秒的时间戳即可。因为微秒的时间戳精度并不可靠。浏览器时间精度，准确度最多也只能到毫秒。时间长度，为了标识用户在某一页面的停留时间。

什么地点：用户接触点，Touch Point。对于每个用户接触点。潜在包含了两层信息：网址 + 内容。

网址：每一个url链接（页面/屏幕），即定位了一个互联网页面地址，或者某个产品的特定页面。可以是PC上某电商网站的页面url，也可以是手机上的微博，微信等应用某个功能页面，某款产品应用的特定画面。如，长城红酒单品页，微信订阅号页面，某游戏的过关页。

内容：每个url网址（页面/屏幕）中的内容。可以是单品的相关信息：类别、品牌、描述、属性、网站信息等等。如，红酒，长城，干红，对于每个互联网接触点，其中网址决定了权重；内容决定了标签。
具体的权重计算公式是：





tag_tf为用户购买商品的成本，tag_action为用户活动，tag_attenuation为用户活动的时间也是衰减因子 ，n为用户购买商品所需要的时间。通过log和数据库的数据可以得到这些数据。

对画像权重的系数ɑ，需要采用逻辑回归的Cost Function进行计算。可以通过matlab来计算θ，确定一个标签的值，如价格{22,11,33，。。。}，然后带入公式进行计算。
逻辑回归的Cost Function可以表示为：
由于y只能等于0或1，所以可以将逻辑回归中的Cost function的两个公式合并，具体推导如下：
故逻辑回归的Cost function可简化为：
对于这个公式，这里稍微补充一点，注意中括号中的公式正是对逻辑回归进行最大似然估计中的最大似然函数，对于最大似然函数求最大值，从而得到参数(\theta\)的估计值。反过来，这里为了求一个合适的参数，需要最小化Cost function，也就是：
minθJ(θ)

而对于新的变量x来说，就是根据hθ(x)的公式输出结果：
与线性回归相似，这里我们采用梯度下降算法来学习参数θ，对于J(θ):
目标是最小化J(θ)，则梯度下降算法的如下：
对J(θ)求导后，梯度下降算法如下：
注意，这个算法和线性回归里的梯度下降算法几乎是一致的，除了hθ(x)的表示不同。
2．逻辑回归算法构建过程
所用数据需要清洗、结构化预处理，再归一化，去除噪音数据，然后通过大数据可视化分析维度的特点，再选择合适的算法模型进行处理。根据用户的行为日志，建立用户的品牌偏好，并预测他们在将来一个月内对品牌下商品的购买行为。将这个问题转为一个二分类问题，可以采用现有的有监督的分类回归算法（逻辑回归、SVM、随机森林、GBRT）等，最后基于基本的算法模型进行模型融合。将训练集分为“购买过”跟“未购买过”两部分，分别训练，对预测集同样分为两部分，分别预测，最后两部分融合。
· 为了训练集和预测集的信息量差异，将不同时间尺度的训练集预测集分别训练预测，最后融合。
· 逻辑回归模型与树模型采用不一样的特征体系，使得融合效果更好。
解决步骤
· 构建特征
· 构建训练集和预测集
· 数据预处理
· 构建本地评估框架
· 模型融合
特征抽取
训练样本的基本形式是一对有交互的用户品牌对，我们对这样的用户品牌对提取特征。首先，从总的分类上分为用户的特征、品牌的特征、用户对这个品牌的特征；其次，考虑用户行为的周期性，从时间维度上分为最近一个星期、半个月、一个月等不同时间段的特征；然后，根据这次比赛给定的四种行为（点击、购买、收藏、购物车），从类型维度上去提取特征。在多维度考虑的基础上，主要提取的是点击数、天数、订单数这样的特征。
这次比赛的目的是由给定的数据去促进业务的提高，从数据到业务需要算法以及特征体系的驱动，在特征抽取的过程中，最本质的一点是要去让数据“贴合”业务，反过来就要求我们从业务指标出发提取特征。参照一些电商分析网站以及自己对业务的理解，诸如用户购买力、品牌购买成本、转化率、用户对品牌的忠诚度等等指标都可以从给定的数据集得以体现。典型的，比如如何衡量品牌的购买成本，可以使用购买该品牌的用户数、回头客的数量、平均每个回头客的回头次数等等来体现这个指标。
特征抽取的最后一部分主要是在之前特征体系的基础上做一些补充，主要有：
1. 比值类特征。主要是一些基本特征的相除，这个一般是具有直观含义的，比如用户购买某个品牌的次数除以用户总的购买次数，这可以体现这个品牌在用户整个购买行为中的地位。
2. 均值、方差类特征。主要是一些购买次数、活跃天数等统计值的平均值或方差，可以体现某些用户某些行为的平均水平或者波动情况。
3. 策略。Season1的时候更多的考虑通过策略来预测，Season2的时候把一部分策略转化为特征，比如最后一次购买之后的点击次数等。
模型训练
这次比赛我们团队主要使用并且起作用的模型有三个：逻辑回归、随机森林和GBRT，逻辑回归本质是一个线性模型，如果为了防止过拟合，可以添加正则化项；如果方便特征的筛选，可以使用逐步逻辑回归；逻辑回归在大数据量下精度会下降，可以通过添加大量的特征（比如哑变量的方式）来提高精度，这次比赛来看，跟随机森林和GBRT相比，逻辑回归单模型上存在天然的劣势。随机森林跟GBRT都是一种ensemble方法，随机森林是一种多棵决策树组合，最后bagging的方法；GBRT是一种boosting方法，每棵树学习的是上一棵树的残差。经过学习之后的LR分类器其实就是一组权值w0,w1,...,wm。

对于模型的训练我们团队主要有两点心得：
· 训练集和预测集的构造。我们通过前三个月提取特征，最后一个月标记来构造训练集；在预测的时候，我们使用四个月所有的行为构造预测集，跟训练集相比，时间尺度上不一样，但是这样会带来更多信息量、更多的用户品牌对，比三个月预测的效果好一些。在处理训练集和预测集时间尺度上不一致时，需要进行数据规格化处理，这里我们使用的是归一化方法（仅仅归一化时间尺度不一致的特征）。归一化方法对离群点比较敏感，这个过程中，需要对数据进行去噪，从而保证训练集跟预测集特征分布的一致性。去噪时，我们去除了异常点击量和异常购买量的用户品牌对。
· 健壮的本地测试。本地测试时，我们尝试过两种方法，一种是将原始的训练样本集划分，80%训练，20%用来本地测试，这种方法跟实际线上训练预测的关系不一样，并且训练集跟测试集的特征分布式一致的，很多问题不易察觉。另外一种方法，本地最后一个月有购买行为的用户品牌对用来测试，前三个月的数据按照线上训练预测的方式构造训练集跟预测集，这种方式时间跨度跟少一个月，但是正样本的数量以及跟线上实际情况很接近，我们主要使用这种方法来本地调参以及部分特征的调试。


首先需要获取最佳的评估参数，使得训练得到的LR模型可以获得最佳的分类效果。这个过程也可以看做是一个搜索的过程，即在一个LR模型的解空间内，如何查找一个与我们设计的LR模型最为匹配的解。为了达到能够获取对应的最佳LR模型，我们需要设计一种搜索策略，考虑按照什么样的准则去选择最优的模型。

如何选择最佳的LR模型，直观的想法就是通过预测模型的结果与真实值的匹配程度评价预测模型的好坏。在机器学习领域中，使用损失函数(loss function)或者代价函数(cost function)来计算预测结果与真实值得匹配程度。

选取了牛顿-拉斐森迭代算法以及L-BFGS算法作为LR模型的迭代算法。当学习算法迭代完成之后，我们可以获对应各个属性的权重。接下来的任务我们需要对现有属性与响应变量之间的显著性进行检验，针对已有的训练模型对应的属性集进行验证，删除显著性不符合阈值的特征。由于在构建风险函数的时候，使用了MLE方法，因此可以使用Wald Test对于计算得到的参数，进行显著性验证。在使用Wald Test之前，要确保期望值与评估值之间的差值符合正态分布。Wald统计变量的一般形式：

其中

表示评估值，表示期望值，表示评估值方差。在本次试验中我们将原假设设定为，即表示现有的属性与响应变量无相关性，因此本实验的Wald统计值可以表示为：

其中是实际估计的参数值，是的标准方差。由于Wald统计值对应卡方分布，因此可以利用卡方分布计算P值，如果P值大于指定的阈值，那么可以认为原假设成立，即该属性与响应变量是显著不相关，删除该变量，否则保存该变量。在实际的训练过程中，每次验证属性显著性的时候，只挑选P值最大与人为设定的阈值进行比较；如果选择的P值不大于阈值，那么模型训练完毕；否则删除选择的P值对应的属性，更新预测模型。重新学习更新后的预测模型，推测对应的权重值，然后再次对各个属性进行Wald
Test验证。重复上面的过程，直到没有任何变量的Wald Test对应的P值都不大于人为设定的阈值为止。到此整个模型的训练过程结束。

三.常用算法大数据有许多数据挖掘的经典算法，涉及到了决策分类，聚类，回归、链接挖掘，关联挖掘，模式挖掘等等方面。其中经典十大算法为：C4.5，K-Means，SVM，Apriori，EM，PageRank，AdaBoost，KNN，NB和CART。一般数据挖掘算法分为两种，有监督和无监督算法，其中有监督算法主要有逻辑回归、决策树、神经网络等，无监督学习主要包括聚类、最邻近距离、支持向量机等。一般需要综合运用多种算法进行分析，运用较多算法的有逻辑回归、SVM、K-Means、NLP自然语言理解等。

1.C4.5算法。C4.5算法与ID3算法一样，都是数学分类算法，C4.5算法是ID3算法的一个改进。ID3算法采用信息增益进行决策判断，而C4.5采用的是增益率。

详细介绍链接：http://blog.csdn.net/androidlushangderen/article/details/42395865

2.CART算法。CART算法的全称是分类回归树算法，他是一个二元分类，采用的是类似于熵的基尼指数作为分类决策，形成决策树后之后还要进行剪枝，我自己在实现整个算法的时候采用的是代价复杂度算法，

详细介绍链接：http://blog.csdn.net/androidlushangderen/article/details/42558235

2. KNN(K最近邻)算法。给定一些已经训练好的数据，输入一个新的测试数据点，计算包含于此测试数据点的最近的点的分类情况，哪个分类的类型占多数，则此

3.

4. 测试点的分类与此相同，所以在这里,有的时候可以复制不同的分类点不同的权重。近的点的权重大点，远的点自然就小点。

详细介绍链接：http://blog.csdn.net/androidlushangderen/article/details/42613011

4.NaiveBayes(朴素贝叶斯)算法。朴素贝叶斯算法是贝叶斯算法里面一种比较简单的分类算法，用到了一个比较重要的贝叶斯定理，用一句简单的话概括就是条件概率的相互转换推导。

详细介绍链接：http://blog.csdn.net/androidlushangderen/article/details/42680161

5.SVM(支持向量机)算法。支持向量机算法是一种对线性和非线性数据进行分类的方法，非线性数据进行分类的时候可以通过核函数转为线性的情况再处理。其中的一个关键的步骤是搜索最大边缘超平面。

详细介绍链接：http://blog.csdn.net/androidlushangderen/article/details/42780439

6.EM(期望最大化)算法。期望最大化算法，可以拆分为2个算法，1个E-Step期望化步骤,和1个M-Step最大化步骤。他是一种算法框架，在每次计算结果之后，逼近统计模型参数的最大似然或最大后验估计。

详细介绍链接：http://blog.csdn.net/androidlushangderen/article/details/42921789

7.Apriori算法。Apriori算法是关联规则挖掘算法，通过连接和剪枝运算挖掘出频繁项集，然后根据频繁项集得到关联规则，关联规则的导出需要满足最小置信度的要求。

详细介绍链接：http://blog.csdn.net/androidlushangderen/article/details/43059211

8.FP-Tree(频繁模式树)算法。这个算法也有被称为FP-growth算法，这个算法克服了Apriori算法的产生过多侯选集的缺点，通过递归的产生频度模式树，然后对树进行挖掘，后面的过程与Apriori算法一致。

详细介绍链接：http://blog.csdn.net/androidlushangderen/article/details/43234309

9.PageRank(网页重要性/排名)算法。PageRank算法最早产生于Google,核心思想是通过网页的入链数作为一个网页好快的判定标准，如果1个网页内部包含了多个指向外部的链接，则PR值将会被均分，PageRank算法也会遭到Link Span攻击。

详细介绍链接：http://blog.csdn.net/androidlushangderen/article/details/43311943

10.HITS算法。HITS算法是另外一个链接算法，部分原理与PageRank算法是比较相似的，HITS算法引入了权威值和中心值的概念，HITS算法是受用户查询条件影响的，他一般用于小规模的数据链接分析，也更容易遭受到攻击。

详细介绍链接：http://blog.csdn.net/androidlushangderen/article/details/43311943

11.K-Means(K均值)算法。K-Means算法是聚类算法，k在在这里指的是分类的类型数，所以在开始设定的时候非常关键，算法的原理是首先假定k个分类点，然后根据欧式距离计算分类，然后去同分类的均值作为新的聚簇中心，循环操作直到收敛。

详细介绍链接：http://blog.csdn.net/androidlushangderen/article/details/43373159

12.BIRCH算法。BIRCH算法利用构建CF聚类特征树作为算法的核心，通过树的形式，BIRCH算法扫描数据库，在内存中建立一棵初始的CF-树，可以看做数据的多层压缩。

详细介绍链接：http://blog.csdn.net/androidlushangderen/article/details/43532111

13.AdaBoost算法。AdaBoost算法是一种提升算法，通过对数据的多次训练得到多个互补的分类器，然后组合多个分类器，构成一个更加准确的分类器。

详细介绍链接：http://blog.csdn.net/androidlushangderen/article/details/43635115

14.GSP算法。GSP算法是序列模式挖掘算法。GSP算法也是Apriori类算法，在算法的过程中也会进行连接和剪枝操作，不过在剪枝判断的时候还加上了一些时间上的约束等条件。

详细介绍链接：http://blog.csdn.net/androidlushangderen/article/details/43699083

15.PreFixSpan算法。PreFixSpan算法是另一个序列模式挖掘算法，在算法的过程中不会产生候选集，给定初始前缀模式，不断的通过后缀模式中的元素转到前缀模式中，而不断的递归挖掘下去。

详细介绍链接：http://blog.csdn.net/androidlushangderen/article/details/43766253

16.CBA(基于关联规则分类)算法。CBA算法是一种集成挖掘算法，因为他是建立在关联规则挖掘算法之上的，在已有的关联规则理论前提下，做分类判断，只是在算法的开始时对数据做处理，变成类似于事务的形式。

详细介绍链接：http://blog.csdn.net/androidlushangderen/article/details/43818787

17.RoughSets(粗糙集)算法。粗糙集理论是一个比较新颖的数据挖掘思想。这里使用的是用粗糙集进行属性约简的算法，通过上下近似集的判断删除无效的属性，进行规制的输出。

详细介绍链接：http://blog.csdn.net/androidlushangderen/article/details/43876001

18.gSpan算法。gSpan算法属于图挖掘算法领域。，主要用于频繁子图的挖掘，相较于其他的图算法，子图挖掘算法是他们的一个前提或基础算法。gSpan算法用到了DFS编码，和Edge五元组，最右路径子图扩展等概念，算法比较的抽象和复杂。

19神经网络算法。BP（Back Propagation）神经网络是1986年由Rumelhart和McCelland为首的科学家小组提出，是一种按误差逆传播算法训练的多层前馈网络，是目前应用最广泛的神经网络模型之一。BP网络能学习和存贮大量的输入-输出模式映射关系，而无需事前揭示描述这种映射关系的数学方程。它的学习规则是使用最速下降法，通过反向传播来不断调整网络的权值和阈值，使网络的误差平方和最小。BP神经网络模型拓扑结构包括输入层（input）、隐层(hidden
layer)和输出层(output layer)。

20线性回归，logistic回归，泊松回归

算法类似性根据算法的功能和形式的类似性，我们可以把算法分类，比如说基于树的算法，基于神经网络的算法等等。当然，机器学习的范围非常庞大，有些算法很难明确归类到某一类。而对于有些分类来说，同一分类的算法可以针对不同类型的问题。这里，我们尽量把常用的算法按照最容易理解的方式进行分类。

回归算法

回归算法是试图采用对误差的衡量来探索变量之间的关系的一类算法。回归算法是统计机器学习的利器。在机器学习领域，人们说起回归，有时候是指一类问题，有时候是指一类算法，这一点常常会使初学者有所困惑。常见的回归算法包括：最小二乘法（OrdinaryLeast Square），逻辑回归（LogisticRegression），逐步式回归（StepwiseRegression），多元自适应回归样条（MultivariateAdaptive
Regression Splines）以及本地散点平滑估计（LocallyEstimated Scatterplot Smoothing）

基于实例的算法

基于实例的算法常常用来对决策问题建立模型，这样的模型常常先选取一批样本数据，然后根据某些近似性把新数据与样本数据进行比较。通过这种方式来寻找最佳的匹配。因此，基于实例的算法常常也被称为“赢家通吃”学习或者“基于记忆的学习”。常见的算法包括k-Nearest Neighbor(KNN), 学习矢量量化（LearningVector Quantization，LVQ），以及自组织映射算法（Self-OrganizingMap，SOM）

正则化方法

正则化方法是其他算法（通常是回归算法）的延伸，根据算法的复杂度对算法进行调整。正则化方法通常对简单模型予以奖励而对复杂算法予以惩罚。常见的算法包括：RidgeRegression，Least Absolute Shrinkage and Selection Operator（LASSO），以及弹性网络（ElasticNet）。



决策树学习

决策树算法根据数据的属性采用树状结构建立决策模型，决策树模型常常用来解决分类和回归问题。常见的算法包括：分类及回归树（ClassificationAnd Regression Tree，CART），ID3 (Iterative Dichotomiser 3)， C4.5，Chi-squared Automatic Interaction Detection(CHAID), Decision
Stump,随机森林（RandomForest），多元自适应回归样条（MARS）以及梯度推进机（GradientBoosting Machine，GBM）



贝叶斯方法

贝叶斯方法算法是基于贝叶斯定理的一类算法，主要用来解决分类和回归问题。常见算法包括：朴素贝叶斯算法，平均单依赖估计（AveragedOne-Dependence Estimators，AODE），以及BayesianBelief Network（BBN）。

基于核的算法

基于核的算法中最著名的莫过于支持向量机（SVM）了。基于核的算法把输入数据映射到一个高阶的向量空间，在这些高阶向量空间里，有些分类或者回归问题能够更容易的解决。常见的基于核的算法包括：支持向量机（SupportVector Machine，SVM），径向基函数（RadialBasis Function，RBF)，以及线性判别分析（Linear Discriminate Analysis，LDA)等。SVM适用于数据量小、维度高、数据不相关的场景。

聚类算法

聚类，就像回归一样，有时候人们描述的是一类问题，有时候描述的是一类算法。聚类算法通常按照中心点或者分层的方式对输入数据进行归并。所以的聚类算法都试图找到数据的内在结构，以便按照最大的共同点将数据进行归类。常见的聚类算法包括k-Means算法以及期望最大化算法（ExpectationMaximization，EM）。

关联规则学习

关联规则学习通过寻找最能够解释数据变量之间关系的规则，来找出大量多元数据集中有用的关联规则。常见算法包括Apriori算法和Eclat算法等。



人工神经网络



人工神经网络算法模拟生物神经网络，是一类模式匹配算法。通常用于解决分类和回归问题。人工神经网络是机器学习的一个庞大的分支，有几百种不同的算法。（其中深度学习就是其中的一类算法，我们会单独讨论），重要的人工神经网络算法包括：感知器神经网络（PerceptronNeural Network）,反向传递（BackPropagation），Hopfield网络，自组织映射（Self-OrganizingMap,
SOM）。学习矢量量化（LearningVector Quantization，LVQ）



深度学习

深度学习算法是对人工神经网络的发展。在近期赢得了很多关注，特别是百度也开始发力深度学习后，更是在国内引起了很多关注。 在计算能力变得日益廉价的今天，深度学习试图建立大得多也复杂得多的神经网络。很多深度学习的算法是半监督式学习算法，用来处理存在少量未标识数据的大数据集。常见的深度学习算法包括：受限波尔兹曼机（RestrictedBoltzmann
Machine，RBN）， DeepBelief Networks（DBN），卷积网络（ConvolutionalNetwork）,堆栈式自动编码器（StackedAuto-encoders）。



降低维度算法

像聚类算法一样，降低维度算法试图分析数据的内在结构，不过降低维度算法是以非监督学习的方式试图利用较少的信息来归纳或者解释数据。这类算法可以用于高维数据的可视化或者用来简化数据以便监督式学习使用。常见的算法包括：主成份分析（PrincipleComponent Analysis，PCA），偏最小二乘回归（PartialLeast Square Regression，PLS）， Sammon映射，多维尺度（Multi-DimensionalScaling,
MDS）, 投影追踪（ProjectionPursuit）等。



集成算法

集成算法用一些相对较弱的学习模型独立地就同样的样本进行训练，然后把结果整合起来进行整体预测。集成算法的主要难点在于究竟集成哪些独立的较弱的学习模型以及如何把学习结果整合起来。这是一类非常强大的算法，同时也非常流行。常见的算法包括：Boosting，Bootstrapped Aggregation（Bagging），AdaBoost，堆叠泛化（StackedGeneralization，Blending），梯度推进机（GradientBoosting
Machine, GBM），随机森林（RandomForest）。
